FROM apache/airflow:2.7.1-python3.11

# Switch to root to install system dependencies
USER root

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      gcc \
      python3-dev \
      openjdk-11-jdk \
      procps && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME for Spark
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH="$JAVA_HOME/bin:$PATH"

# Copy requirements file into image
COPY requirements.txt /requirements.txt

# Switch to airflow user (which should have UID=50000 per .env)
USER airflow

# Install additional Python packages (do NOT use --user)
RUN pip install --no-cache-dir -r /requirements.txt

# (Optional) Set a working directory
WORKDIR /opt/airflow